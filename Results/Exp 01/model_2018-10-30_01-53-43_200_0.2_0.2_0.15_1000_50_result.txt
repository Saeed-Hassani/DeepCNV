> Training data results: 
loss: 0.0032478539942
acc: 0.990301724138

> Accracy:
 acc: 96.26% 

 
   32/2784 [..............................] - ETA: 1:09
  64/2784 [..............................] - ETA: 1:01
  96/2784 [>.............................] - ETA: 42s 
 128/2784 [>.............................] - ETA: 33s
 160/2784 [>.............................] - ETA: 27s
 192/2784 [=>............................] - ETA: 23s
 224/2784 [=>............................] - ETA: 21s
 256/2784 [=>............................] - ETA: 19s
 288/2784 [==>...........................] - ETA: 17s
 320/2784 [==>...........................] - ETA: 16s
 352/2784 [==>...........................] - ETA: 15s
 384/2784 [===>..........................] - ETA: 14s
 416/2784 [===>..........................] - ETA: 13s
 448/2784 [===>..........................] - ETA: 12s
 480/2784 [====>.........................] - ETA: 12s
 512/2784 [====>.........................] - ETA: 11s
 544/2784 [====>.........................] - ETA: 11s
 576/2784 [=====>........................] - ETA: 10s
 608/2784 [=====>........................] - ETA: 10s
 640/2784 [=====>........................] - ETA: 10s
 672/2784 [======>.......................] - ETA: 9s 
 704/2784 [======>.......................] - ETA: 9s
 736/2784 [======>.......................] - ETA: 9s
 768/2784 [=======>......................] - ETA: 9s
 800/2784 [=======>......................] - ETA: 8s
 832/2784 [=======>......................] - ETA: 8s
 864/2784 [========>.....................] - ETA: 8s
 896/2784 [========>.....................] - ETA: 8s
 928/2784 [=========>....................] - ETA: 7s
 960/2784 [=========>....................] - ETA: 7s
 992/2784 [=========>....................] - ETA: 7s
1024/2784 [==========>...................] - ETA: 7s
1056/2784 [==========>...................] - ETA: 6s
1088/2784 [==========>...................] - ETA: 6s
1120/2784 [===========>..................] - ETA: 6s
1152/2784 [===========>..................] - ETA: 6s
1184/2784 [===========>..................] - ETA: 6s
1216/2784 [============>.................] - ETA: 5s
1248/2784 [============>.................] - ETA: 5s
1280/2784 [============>.................] - ETA: 5s
1312/2784 [=============>................] - ETA: 5s
1344/2784 [=============>................] - ETA: 5s
1376/2784 [=============>................] - ETA: 5s
1408/2784 [==============>...............] - ETA: 4s
1440/2784 [==============>...............] - ETA: 4s
1472/2784 [==============>...............] - ETA: 4s
1504/2784 [===============>..............] - ETA: 4s
1536/2784 [===============>..............] - ETA: 4s
1568/2784 [===============>..............] - ETA: 4s
1600/2784 [================>.............] - ETA: 4s
1632/2784 [================>.............] - ETA: 4s
1664/2784 [================>.............] - ETA: 3s
1696/2784 [=================>............] - ETA: 3s
1728/2784 [=================>............] - ETA: 3s
1760/2784 [=================>............] - ETA: 3s
1792/2784 [==================>...........] - ETA: 3s
1824/2784 [==================>...........] - ETA: 3s
1856/2784 [===================>..........] - ETA: 3s
1888/2784 [===================>..........] - ETA: 3s
1920/2784 [===================>..........] - ETA: 2s
1952/2784 [====================>.........] - ETA: 2s
1984/2784 [====================>.........] - ETA: 2s
2016/2784 [====================>.........] - ETA: 2s
2048/2784 [=====================>........] - ETA: 2s
2080/2784 [=====================>........] - ETA: 2s
2112/2784 [=====================>........] - ETA: 2s
2144/2784 [======================>.......] - ETA: 2s
2176/2784 [======================>.......] - ETA: 1s
2208/2784 [======================>.......] - ETA: 1s
2240/2784 [=======================>......] - ETA: 1s
2272/2784 [=======================>......] - ETA: 1s
2304/2784 [=======================>......] - ETA: 1s
2336/2784 [========================>.....] - ETA: 1s
2368/2784 [========================>.....] - ETA: 1s
2400/2784 [========================>.....] - ETA: 1s
2432/2784 [=========================>....] - ETA: 1s
2464/2784 [=========================>....] - ETA: 1s
2496/2784 [=========================>....] - ETA: 0s
2528/2784 [==========================>...] - ETA: 0s
2560/2784 [==========================>...] - ETA: 0s
2592/2784 [==========================>...] - ETA: 0s
2624/2784 [===========================>..] - ETA: 0s
2656/2784 [===========================>..] - ETA: 0s
2688/2784 [===========================>..] - ETA: 0s
2720/2784 [============================>.] - ETA: 0s
2752/2784 [============================>.] - ETA: 0s
2784/2784 [==============================] - 9s 3ms/step
> Training data results: 
loss: 0.0032478539942
acc: 0.990301724138
> Saved model to disk
> Loaded model from disk
> acc: 96.26%
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_1 (LSTM)                (None, 200)               13266400  
_________________________________________________________________
dropout_1 (Dropout)          (None, 200)               0         
_________________________________________________________________
dense_1 (Dense)              (None, 5)                 1005      
=================================================================
Total params: 13,267,405
Trainable params: 13,267,405
Non-trainable params: 0 


_________________________________________________________________
code:
_________________________________________________________________

import matplotlib.pyplot as plt
import numpy as np
import pandas as pa
from keras.layers import Dense
from keras.layers import Dropout
from keras.layers import LSTM
from keras.models import Sequential
from keras.models import model_from_json
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from time import gmtime, strftime
from keras.callbacks import CSVLogger

# model_[LSTM units]_[Dropout percent]_[test_size]_[Validation]_[Epochs]_[Batch Size]
_lstm_units = 200
_dropout = .2
_test_sizze = .2
_validation = 0.15
_epochs = 1000
_batch_size = 50
_now = strftime("%Y-%m-%d_%H-%M-%S_", gmtime())
_model_name = "model_" + _now + str(_lstm_units) + "_" + str(_dropout) + "_" + str(_test_sizze) + "_" + str(
    _validation) + "_" + str(_epochs) + "_" + str(
    _batch_size)


def one_hot(_c_name):
    """
        Define the encoder function.
        For example, if we've 3 classes, A B C
        for input [A B C] the output is as follow:
        _c_name =  [1 0 0
                    0 1 0
                    0 0 1]
    """
    _n_class = len(_c_name)
    _unique_labels = len(np.unique(_c_name))
    _output = np.zeros((_n_class, _unique_labels))
    _output[np.arange(_n_class), _c_name] = 1
    return _output


_raw_data = pa.read_csv("D:\\Thesis\\DeepCNV\\final_dataset.csv", low_memory=False)
X = _raw_data[_raw_data.columns[1:_raw_data.shape[1] - 1]].values
_c_name = _raw_data[_raw_data.columns[_raw_data.shape[1] - 1]]
_encoder = LabelEncoder()
_encoder.fit(_c_name)
_c_name = _encoder.transform(_c_name)
Y = one_hot(_c_name)

train_X, test_X, train_Y, test_Y = train_test_split(X, Y, test_size=_test_sizze, random_state=np.random.seed(7),
                                                    shuffle=True)

train_X = np.reshape(train_X, (train_X.shape[0], 1, train_X.shape[1]))
test_X = np.reshape(test_X, (test_X.shape[0], 1, test_X.shape[1]))

# print(train_X, train_X.shape)
# print(train_Y, train_Y.shape)
model = Sequential()
# Since we know the shape of our Data we can input the time step and feature data
# The number of timestep sequence are dealt with in the fit function
model.add(LSTM(_lstm_units, return_sequences=False, input_shape=(train_X.shape[1], train_X.shape[2])))
model.add(Dropout(_dropout))
# number of features on the output
model.add(Dense(train_Y.shape[1], activation='softmax'))
model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])
csv_logger = CSVLogger(_model_name + '_log.txt', append=True, separator=';')

history = model.fit(train_X, train_Y, validation_split=_validation,
                    epochs=_epochs, batch_size=_batch_size, callbacks=[csv_logger], verbose=0)

# ====ذخیره مدل به صورت JSON========
# serialize model to JSON
model_json = model.to_json()
with open(_model_name + ".json", "w") as json_file:
    json_file.write(model_json)

_file = open(_model_name + "_result.txt", "w")
# ======= محاسبه دقت آموزش ========
metrics = model.evaluate(train_X, train_Y)
print('> Training data results: ')
_file.write('> Training data results: \n')
for i in range(len(model.metrics_names)):
    _st = str(model.metrics_names[i]) + ": " + str(metrics[i])
    _file.write(_st + '\n')
    print(_st)

# ====ذخیره وزن ها به صورت h5 ===========
model.save_weights(_model_name + ".h5")
print("> Saved model to disk")

# ==== تست برنامه =============
# load json and create model
json_file = open(_model_name + '.json', 'r')
loaded_model_json = json_file.read()
json_file.close()
loaded_model = model_from_json(loaded_model_json)
# load weights into new model
loaded_model.load_weights(_model_name + ".h5")
print("> Loaded model from disk")
# evaluate loaded model on test data
loaded_model.compile(loss='mean_squared_error', optimizer='rmsprop',
                     metrics=['accuracy'])
score = loaded_model.evaluate(test_X, test_Y, verbose=0)
_file.write("\n> Accracy:\n %s: %.2f%% \n" % (loaded_model.metrics_names[1], score[1] * 100))
print("> %s: %.2f%%" % (loaded_model.metrics_names[1], score[1] * 100))
_file.close()
# ==== نمایش نتایج ======
# list all data in history
# print(history.history.keys())
# summarize history for accuracy
_fig1 = plt.figure()
plt.plot(history.history['acc'])
plt.plot(history.history['val_acc'])
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['train', 'validation'], loc='upper right')
plt.grid(True, linestyle='-', linewidth=.5)
# plt.show()
plt.savefig(_model_name + '_acc.eps', format='eps', dpi=1000)
plt.close(_fig1)
# summarize history for loss
_fig2 = plt.figure()
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Mean Squared Error')
plt.ylabel('Loss')
plt.grid(True, linestyle='-', linewidth=.5)
plt.xlabel('Epoch')
plt.legend(['train', 'validation'], loc='upper right')
# plt.show()
plt.savefig(_model_name + '_loss.eps', format='eps', dpi=1000)
plt.close(_fig2)
model.summary()
